import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Step 1: Data Preparation
corpus = [
    'I like deep learning',
    'I enjoy natural language processing',
    'I prefer coding in Python',
    'Machine learning is fascinating'
]

# Tokenize the words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1

# Generate training data
input_sequences = []
for line in corpus:
    tokenized_line = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(tokenized_line) - 1):
        context = [tokenized_line[i - 1], tokenized_line[i + 1]]
        target = tokenized_line[i]
        input_sequences.append((context, target))

# Convert the sequences to numpy arrays
x_train = np.array([np.array(seq[0]) for seq in input_sequences])
y_train = np.array([seq[1] for seq in input_sequences])

# Step 2: Build the CBOW model
embedding_dim = 50

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=2),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(total_words, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Step 3: Train the model
model.fit(x_train, y_train, epochs=100, verbose=1)

# Step 4: Output
word_index = tokenizer.word_index
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

# Get the embedding layer weights
embedding_layer = model.layers[0]
weights = embedding_layer.get_weights()[0]

# Function to get the closest words to a given word
def find_closest_words(word, weights, word_index, reverse_word_index, top_n=3):
    index = word_index[word]
    embedding = weights[index]
    distances = np.dot(weights, embedding)
    closest_words_indices = np.argsort(distances)[::-1][1:top_n+1]
    closest_words = [reverse_word_index[idx] for idx in closest_words_indices]
    return closest_words

# Test the model by finding closest words to a given word
test_word = 'learning'
closest_words = find_closest_words(test_word, weights, word_index, reverse_word_index)
print(f"Closest words to '{test_word}': {closest_words}")
